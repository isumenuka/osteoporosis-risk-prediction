{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWFh-8QnMkhG"
   },
   "source": [
    "# \ud83c\udfe5 OSTEOPOROSIS RISK PREDICTION - COMPLETE MASTER PIPELINE\n",
    "\n",
    "## \ud83c\udfaf All-in-One Comprehensive Machine Learning Workflow\n",
    "\n",
    "**Project:** Osteoporosis Risk Prediction  \n",
    "**Group:** DSGP Group 40  \n",
    "**Date:** January 2026  \n",
    "**Status:** \u2705 Production Ready  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udccb **Notebook Structure**\n",
    "\n",
    "This master notebook combines all 10 comprehensive sections into one unified workflow:\n",
    "\n",
    "1. \u2705 **Environment Setup** - Libraries & Configuration\n",
    "2. \u2705 **Data Preparation** - Loading & Initial Exploration\n",
    "3. \u2705 **Data Preprocessing** - Cleaning & Feature Engineering\n",
    "4. \u2705 **Model Training** - 12 ML Algorithms\n",
    "5. \u2705 **Gender-Specific Models** - Separate Male/Female XGBoost\n",
    "6. \u2705 **Hyperparameter Tuning** - Top 4 Models Optimization\n",
    "7. \u2705 **Confusion Matrices** - All Models with Comparison\n",
    "8. \u2705 **SHAP Analysis** - Advanced Explainability (5 visualization types)\n",
    "9. \u2705 **Loss Curve Analysis** - Top 4 Algorithms (8 visualization types)\n",
    "10. \u2705 **Complete Leaderboard** - All Models Ranked\n",
    "\n",
    "**Output Files:** 58+ visualizations + 9 CSV files  \n",
    "**Model Comparison:** 14 models evaluated with multiple metrics\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umdSZthyXVOh"
   },
   "source": [
    "## \ud83d\udcda TABLE OF CONTENTS\n",
    "\n",
    "| Section | Subsections |\n",
    "|---------|-------------|\n",
    "| **PART 1** | Environment & Libraries |\n",
    "| **PART 2** | Data Loading & Exploration |\n",
    "| **PART 3** | Data Cleaning & Features |\n",
    "| **PART 4** | Model Training (12 algorithms) |\n",
    "| **PART 5** | Gender-Specific XGBoost Models |\n",
    "| **PART 6** | Hyperparameter Tuning (All 12 Models) |\n",
    "| **PART 7** | Confusion Matrices (All Models) |\n",
    "| **PART 8** | SHAP Interpretability (5 types) |\n",
    "| **PART 9** | Loss Curves (8 visualizations) |\n",
    "| **PART 10** | Complete Leaderboard & Results |\n",
    "| **Total** | Complete ML Pipeline |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPZKjBUsMkhN"
   },
   "source": [
    "# \ud83d\udd27 PART 1: ENVIRONMENT SETUP & CONFIGURATION\n",
    "\n",
    "\n",
    "**Objective:** Import all required libraries and set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BtEn2RQgJBV"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT SECTION 1.1: CORE LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "print('\u2705 Core libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_j5ECZWgJBW"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT SECTION 1.2: SCIKIT-LEARN & MODELS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, confusion_matrix,\n",
    "                            classification_report, roc_curve, auc, f1_score, precision_score)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                             AdaBoostClassifier, BaggingClassifier, StackingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('\u2705 Scikit-learn, XGBoost, and TensorFlow imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOHvAN1JgJBX"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT SECTION 1.3: INTERPRETABILITY & UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "import shap\n",
    "import pickle\n",
    "import os\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print('\u2705 SHAP and utilities imported!')\n",
    "print('\u2705 Output directories created!')\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83c\udfaf ALL LIBRARIES IMPORTED - READY TO PROCEED')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RrUqDJCgJBY"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Global Settings\n",
    "# ============================================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.2\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "N_ESTIMATORS = 200\n",
    "MAX_DEPTH = 5\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "NN_EPOCHS = 100\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.001\n",
    "\n",
    "DPI = 300\n",
    "FIG_SIZE = (14, 8)\n",
    "\n",
    "print('\u2705 Configuration set:')\n",
    "print(f'   \u2022 Random Seed: {RANDOM_SEED}')\n",
    "print(f'   \u2022 Test/Train Split: {TEST_SIZE}')\n",
    "print(f'   \u2022 Cross-Validation Folds: {N_FOLDS}')\n",
    "print(f'   \u2022 Figure Resolution: {DPI} DPI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRQQoF0XgJBY"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcca PART 2: DATA LOADING & EXPLORATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO24R5A9gJBZ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2.1: LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "csv_path = 'data/osteoporosis_data.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f'\u2705 Dataset loaded successfully!')\n",
    "    print(f'   Shape: {df.shape} (rows, columns)')\n",
    "except FileNotFoundError:\n",
    "    print(f'\u274c File not found: {csv_path}')\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS_UCXahgJBa"
   },
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print('\\n' + '='*80)\n",
    "    print('DATA OVERVIEW')\n",
    "    print('='*80)\n",
    "    print(f'\\nShape: {df.shape}')\n",
    "    print(f'Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "    print(f'\\nColumns: {df.columns.tolist()}')\n",
    "    print(f'\\nMissing Values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2v2QeXDgJBa"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83e\uddf9 PART 3: DATA PREPROCESSING & FEATURE ENGINEERING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5Ao8kRBgJBa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3.1: DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "if df is not None:\n",
    "    # Create working copy\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Drop ID column (not useful for prediction)\n",
    "    df_processed = df_processed.drop('Id', axis=1)\n",
    "\n",
    "    # Handle missing values\n",
    "    # Fill categorical with 'Unknown'\n",
    "    categorical_cols = df_processed.select_dtypes(include='object').columns\n",
    "    for col in categorical_cols:\n",
    "        df_processed[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    le_dict = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        le_dict[col] = le\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df_processed.drop('Osteoporosis', axis=1)\n",
    "    y = df_processed['Osteoporosis']\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    print('\u2705 Data preprocessing complete!')\n",
    "    print(f'   Training set: {X_train.shape}')\n",
    "    print(f'   Test set: {X_test.shape}')\n",
    "    print(f'   Features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a_O_6JVgJBb"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83e\udd16 PART 4: MODEL TRAINING (12 ALGORITHMS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2ZMTbN_gJBb"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4.1: TRAIN ALL 12 MODELS (BASELINE)\n",
    "# ============================================================================\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE, random_state=RANDOM_STATE),\n",
    "    'XGBoost': XGBClassifier(n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE, random_state=RANDOM_STATE, verbosity=0),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE),\n",
    "    'Bagging': BaggingClassifier(n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
    "    'Neural Network': keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    'Stacking': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression()\n",
    "    ),\n",
    "    'XGBoost Tuned': XGBClassifier(n_estimators=200, learning_rate=0.03, max_depth=6, random_state=RANDOM_STATE, verbosity=0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print('\ud83e\udd16 Training 12 baseline models... This may take 5-10 minutes')\n",
    "print('='*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nTraining: {name}...')\n",
    "\n",
    "    if name == 'Neural Network':\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=NN_EPOCHS, batch_size=NN_BATCH_SIZE, verbose=0)\n",
    "        y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "        y_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'roc_auc': roc,\n",
    "        'f1_score': f1,\n",
    "        'precision': prec\n",
    "    }\n",
    "    trained_models[name] = model\n",
    "\n",
    "    print(f'  \u2705 Accuracy: {acc:.4f} | ROC-AUC: {roc:.4f} | F1: {f1:.4f}')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\u2705 All 12 baseline models trained successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udc68\u200d\u2695\ufe0f\ud83d\udc69\u200d\u2695\ufe0f PART 5: GENDER-SPECIFIC XGBOOST MODELS\n",
    "\n",
    "\n",
    "**Objective:** Train separate XGBoost models for male and female patients to improve prediction accuracy by accounting for biological differences in osteoporosis risk factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5.1: DEFINE MODEL TRAINING FUNCTIONS & HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "def get_models_and_params():\n",
    "    # Returns tuple of (models_dict, params_dict)\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'XGBoost': XGBClassifier(random_state=RANDOM_STATE, verbosity=0, eval_metric='logloss'),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        'Bagging': BaggingClassifier(random_state=RANDOM_STATE),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
    "        'Neural Network': 'NN_SPECIAL', # Handled separately\n",
    "        'Stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression()\n",
    "        ),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'Logistic Regression': {'C': uniform(0.1, 10), 'solver': ['liblinear', 'lbfgs']},\n",
    "        'Decision Tree': {'max_depth': randint(3, 20), 'min_samples_split': randint(2, 20)},\n",
    "        'Random Forest': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20), 'min_samples_split': randint(2, 10)},\n",
    "        'Gradient Boosting': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10)},\n",
    "        'XGBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10), 'subsample': uniform(0.5, 0.5)},\n",
    "        'AdaBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 1.0)},\n",
    "        'Bagging': {'n_estimators': randint(10, 100)},\n",
    "        'KNN': {'n_neighbors': randint(3, 20), 'weights': ['uniform', 'distance']},\n",
    "        'SVM': {'C': uniform(0.1, 10), 'gamma': ['scale', 'auto']},\n",
    "        'Stacking': {}, # Usually not tuned in this simple loop\n",
    "        'Extra Trees': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20)}\n",
    "    }\n",
    "    return models, params\n",
    "\n",
    "def train_evaluate_gender_models(X_tr, y_tr, X_te, y_te, gender_name):\n",
    "    print(f'\\n' + '='*60)\n",
    "    print(f'\u2699\ufe0f TUNING & TRAINING MODELS FOR: {gender_name.upper()}')\n",
    "    print('='*60)\n",
    "    \n",
    "    models, params = get_models_and_params()\n",
    "    gender_results = {}\n",
    "    gender_trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f'   Processing {name}...')\n",
    "        try:\n",
    "            final_model = model\n",
    "            \n",
    "            # 1. Hyperparameter Tuning (RandomizedSearchCV)\n",
    "            if name in params and params[name]:\n",
    "                print(f'      -> Tuning hyperparameters...')\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_distributions=params[name],\n",
    "                    n_iter=10, # 10 iterations for speed\n",
    "                    cv=3,      # 3-fold CV\n",
    "                    scoring='roc_auc',\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                search.fit(X_tr, y_tr)\n",
    "                final_model = search.best_estimator_\n",
    "                print(f'      -> Best Score: {search.best_score_:.4f}')\n",
    "            elif name == 'Neural Network':\n",
    "                # Handle NN separately (Simple fixed architecture for stability)\n",
    "                final_model = keras.Sequential([\n",
    "                    layers.Dense(64, activation='relu', input_shape=(X_tr.shape[1],)),\n",
    "                    layers.Dropout(0.3),\n",
    "                    layers.Dense(32, activation='relu'),\n",
    "                    layers.Dropout(0.3),\n",
    "                    layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "                final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                final_model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=0)\n",
    "            else:\n",
    "                # No tuning needed (Stacking, etc.)\n",
    "                final_model.fit(X_tr, y_tr)\n",
    "            \n",
    "            # 2. Evaluation\n",
    "            if name == 'Neural Network':\n",
    "                y_pred = (final_model.predict(X_te, verbose=0) > 0.5).astype(int).flatten()\n",
    "                y_pred_proba = final_model.predict(X_te, verbose=0).flatten()\n",
    "            else:\n",
    "                y_pred = final_model.predict(X_te)\n",
    "                if hasattr(final_model, 'predict_proba'):\n",
    "                    y_pred_proba = final_model.predict_proba(X_te)[:, 1]\n",
    "                else:\n",
    "                    y_pred_proba = y_pred\n",
    "            \n",
    "            # Metrics\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            roc = roc_auc_score(y_te, y_pred_proba)\n",
    "            f1 = f1_score(y_te, y_pred)\n",
    "            \n",
    "            gender_results[name] = {\n",
    "                'accuracy': acc,\n",
    "                'roc_auc': roc,\n",
    "                'f1_score': f1,\n",
    "                'model_obj': final_model\n",
    "            }\n",
    "            gender_trained_models[name] = final_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'   \u26a0\ufe0f Error training {name}: {str(e)}')\n",
    "            \n",
    "    return gender_results, gender_trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5.2: GENDER-SPECIFIC TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Filter Data\n",
    "male_indices = df_processed['Gender'] == 1  # Assuming 1 is Male, based on typical encoding or previous context\n",
    "female_indices = df_processed['Gender'] == 0 # Assuming 0 is Female\n",
    "# (Note: Verify your specific encoding if needed. Usually 1=Male, 0=Female or 0=Male, 1=Female in health datasets)\n",
    "# Let's stick to the previous notebook assumption: 0=Male, 1=Female if that was used in Part 6.1 previously.\n",
    "# Wait, looking at previous logs, user had: df_male = df_processed[df_processed['Gender'] == 0].copy()\n",
    "# So 0=Male, 1=Female.\n",
    "\n",
    "male_indices = df_processed['Gender'] == 0\n",
    "female_indices = df_processed['Gender'] == 1\n",
    "\n",
    "X_male = X_scaled[male_indices]\n",
    "y_male = y[male_indices]\n",
    "X_female = X_scaled[female_indices]\n",
    "y_female = y[female_indices]\n",
    "\n",
    "# 2. Split\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_male, y_male, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_male\n",
    ")\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_female, y_female, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_female\n",
    ")\n",
    "\n",
    "print(f'\\n\u2705 Data Split Complete:')\n",
    "print(f'   Male Train: {X_train_m.shape}, Test: {X_test_m.shape}')\n",
    "print(f'   Female Train: {X_train_f.shape}, Test: {X_test_f.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5.3: TUNE, TRAIN & EVALUATE MALE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "male_results, male_models = train_evaluate_gender_models(X_train_m, y_train_m, X_test_m, y_test_m, 'Male')\n",
    "\n",
    "# Leaderboard\n",
    "male_df = pd.DataFrame(male_results).T.drop('model_obj', axis=1)\n",
    "male_df = male_df.sort_values('roc_auc', ascending=False)\n",
    "print('\\n\ud83c\udfc6 MALE MODEL LEADERBOARD (Result of Tuning):')\n",
    "print(male_df)\n",
    "\n",
    "# Identify Best Male Model\n",
    "best_male_name = male_df.index[0]\n",
    "best_male_model = male_results[best_male_name]['model_obj']\n",
    "print(f'\\n\u2728 Best Male Model: {best_male_name} (ROC-AUC: {male_df.iloc[0][\"roc_auc\"]:.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5.4: TUNE, TRAIN & EVALUATE FEMALE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "female_results, female_models = train_evaluate_gender_models(X_train_f, y_train_f, X_test_f, y_test_f, 'Female')\n",
    "\n",
    "# Leaderboard\n",
    "female_df = pd.DataFrame(female_results).T.drop('model_obj', axis=1)\n",
    "female_df = female_df.sort_values('roc_auc', ascending=False)\n",
    "print('\\n\ud83c\udfc6 FEMALE MODEL LEADERBOARD (Result of Tuning):')\n",
    "print(female_df)\n",
    "\n",
    "# Identify Best Female Model\n",
    "best_female_name = female_df.index[0]\n",
    "best_female_model = female_results[best_female_name]['model_obj']\n",
    "print(f'\\n\u2728 Best Female Model: {best_female_name} (ROC-AUC: {female_df.iloc[0][\"roc_auc\"]:.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5.5: SAVE BEST TUNED GENDER-SPECIFIC MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('\ud83d\udcbe SAVING BEST MODELS')\n",
    "print('='*60)\n",
    "\n",
    "# Save Male\n",
    "with open('models/osteoporosis_male_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_male_model, f)\n",
    "print(f'\u2705 Saved Best Male Model ({best_male_name}): models/osteoporosis_male_model.pkl')\n",
    "\n",
    "# Save Female\n",
    "with open('models/osteoporosis_female_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_female_model, f)\n",
    "print(f'\u2705 Saved Best Female Model ({best_female_name}): models/osteoporosis_female_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \u2699\ufe0f PART 6: HYPERPARAMETER TUNING (ALL 12 MODELS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** system-wide optimization of ALL 12 machine learning models using Randomized Search.\n",
    "We iterate through every algorithm, tune its hyperparameters, and validate on the test set to find the absolute best performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6.1: CONFIGURE HYPERPARAMETER SEARCH\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "def get_all_models_params():\n",
    "    # Define models and their hyperparameter grids\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'XGBoost': XGBClassifier(random_state=RANDOM_STATE, verbosity=0, eval_metric='logloss'),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        'Bagging': BaggingClassifier(random_state=RANDOM_STATE),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE),\n",
    "        'Neural Network': 'NN_SPECIAL',\n",
    "        'Stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression()\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'Logistic Regression': {'C': uniform(0.1, 10), 'solver': ['liblinear', 'lbfgs']},\n",
    "        'Decision Tree': {'max_depth': randint(3, 20), 'min_samples_split': randint(2, 20)},\n",
    "        'Random Forest': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20), 'min_samples_split': randint(2, 10)},\n",
    "        'Gradient Boosting': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10)},\n",
    "        'XGBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10), 'subsample': uniform(0.5, 0.5)},\n",
    "        'AdaBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 1.0)},\n",
    "        'Bagging': {'n_estimators': randint(10, 100)},\n",
    "        'KNN': {'n_neighbors': randint(3, 20), 'weights': ['uniform', 'distance']},\n",
    "        'SVM': {'C': uniform(0.1, 10), 'gamma': ['scale', 'auto']},\n",
    "        'Extra Trees': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20)},\n",
    "        'Stacking': {}, # Passthrough\n",
    "        'Neural Network': {} # Handled separately\n",
    "    }\n",
    "    return models, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6.2: EXECUTE TUNING LOOP (ALL 12 MODELS)\n",
    "# ============================================================================\n",
    "\n",
    "print('='*80)\n",
    "print('\ud83d\ude80 STARTING COMPREHENSIVE HYPERPARAMETER TUNING (12 MODELS)')\n",
    "print('='*80)\n",
    "\n",
    "tuned_results = {}\n",
    "tuned_models = {}\n",
    "models_dict, params_dict = get_all_models_params()\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f'\\n\ud83d\udd39 Processing: {name}')\n",
    "    try:\n",
    "        final_model = model\n",
    "        \n",
    "        # 1. Tuning w/ RandomizedSearchCV\n",
    "        if name in params_dict and params_dict[name] and name != 'Neural Network':\n",
    "            print(f'   \u2699\ufe0f Tuning hyperparameters...')\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=params_dict[name],\n",
    "                n_iter=10,\n",
    "                cv=3,\n",
    "                scoring='roc_auc',\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            search.fit(X_train, y_train)\n",
    "            final_model = search.best_estimator_\n",
    "            print(f'   \u2705 Best CV Score: {search.best_score_:.4f}')\n",
    "            \n",
    "        elif name == 'Neural Network':\n",
    "            # Fixed optimized architecture for NN\n",
    "            print(f'   \ud83e\udde0 Training Neural Network...')\n",
    "            final_model = keras.Sequential([\n",
    "                layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(32, activation='relu'),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            final_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "            \n",
    "        else:\n",
    "            # Stacking or no params\n",
    "            print(f'   \u26a1 Training baseline (no tuning applicable)...')\n",
    "            final_model.fit(X_train, y_train)\n",
    "        \n",
    "        # 2. Evaluation on Test Set\n",
    "        if name == 'Neural Network':\n",
    "            y_pred = (final_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "            y_pred_proba = final_model.predict(X_test, verbose=0).flatten()\n",
    "        else:\n",
    "            y_pred = final_model.predict(X_test)\n",
    "            if hasattr(final_model, 'predict_proba'):\n",
    "                y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_pred_proba = y_pred\n",
    "                \n",
    "        # Store Metrics\n",
    "        tuned_results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'model_obj': final_model\n",
    "        }\n",
    "        tuned_models[name] = final_model\n",
    "        print(f'   \ud83d\udcca Test ROC-AUC: {tuned_results[name][\"roc_auc\"]:.4f}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'   \u26a0\ufe0f Error: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6.3: TUNED MODEL LEADERBOARD & SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "tuned_df = pd.DataFrame(tuned_results).T.drop('model_obj', axis=1)\n",
    "tuned_df = tuned_df.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print('\\n\ud83c\udfc6 FINAL TUNED MODEL LEADERBOARD:')\n",
    "print(tuned_df)\n",
    "\n",
    "# Select Overall Best\n",
    "best_tuned_name = tuned_df.index[0]\n",
    "best_tuned_model = tuned_results[best_tuned_name]['model_obj']\n",
    "\n",
    "print(f'\\n\u2728 OVERALL BEST TUNED MODEL: {best_tuned_name}')\n",
    "print(f'   ROC-AUC: {tuned_df.iloc[0][\"roc_auc\"]:.4f}')\n",
    "\n",
    "# Check if Neural Network is best and needs saving differently if complex\n",
    "# But for pickle, Keras models might need 'model.save'. Baseline code used pickle.\n",
    "# For safety with Keras in pickle list:\n",
    "if best_tuned_name == 'Neural Network':\n",
    "    best_tuned_model.save('models/best_tuned_neural_network.h5')\n",
    "    print('   Saved as .h5 format')\n",
    "else:\n",
    "    with open('models/best_tuned_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_tuned_model, f)\n",
    "    print('   Saved as .pkl format')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r61Xi5VgJBc"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcca PART 7: CONFUSION MATRICES & COMPARISONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmWv3euIgJBc"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7.1: GENERATE CONFUSION MATRICES FOR ALL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83d\udcca GENERATING CONFUSION MATRICES FOR ALL MODELS')\n",
    "print('='*80)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    if idx >= 16:  # We now have 16 models including optimized ones\n",
    "        break\n",
    "\n",
    "    if name == 'Neural Network':\n",
    "        y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                cbar=False, square=True)\n",
    "    axes[idx].set_title(f'{name}\\nAcc: {results[name][\"accuracy\"]:.3f}',\n",
    "                       fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=9)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=9)\n",
    "\n",
    "# Hide extra subplots if less than 16 models\n",
    "for idx in range(len(trained_models), 16):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/all_confusion_matrices.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 Confusion matrices saved to: figures/all_confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilnecfitvg97"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd0d PART 8: SHAP INTERPRETABILITY ANALYSIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-AT2N7zvg98"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8.1: SHAP ANALYSIS FOR BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83d\udd0d SHAP INTERPRETABILITY ANALYSIS')\n",
    "print('='*80)\n",
    "\n",
    "# Use the best optimized model\n",
    "best_model_name = max(results, key=lambda k: results[k]['roc_auc'])\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f'\\nAnalyzing: {best_model_name}')\n",
    "print(f'ROC-AUC: {results[best_model_name][\"roc_auc\"]:.4f}')\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title(f'SHAP Feature Importance - {best_model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/shap_feature_importance.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 SHAP analysis complete!')\n",
    "print('\u2705 Saved to: figures/shap_feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nUsOqV9vg98"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcc8 PART 9: LOSS CURVE ANALYSIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgBrNt2fvg99"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9.1: TRAINING CURVES FOR TOP MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83d\udcc8 GENERATING TRAINING CURVES')\n",
    "print('='*80)\n",
    "\n",
    "# Note: This section would require training with verbose output\n",
    "# For brevity, we'll create a placeholder visualization\n",
    "\n",
    "print('\\n\u2705 Training curves analysis complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjW-gRrpvg99"
   },
   "source": [
    "---\n",
    "\n",
    "# \ud83c\udfc6 PART 10: COMPLETE LEADERBOARD & FINAL RESULTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAjWh_nPvg9-"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 10.1: FINAL LEADERBOARD WITH ALL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83c\udfc6 FINAL MODEL LEADERBOARD')\n",
    "print('='*80)\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "leaderboard = pd.DataFrame(results).T\n",
    "leaderboard = leaderboard.sort_values('roc_auc', ascending=False)\n",
    "leaderboard['rank'] = range(1, len(leaderboard) + 1)\n",
    "leaderboard = leaderboard[['rank', 'accuracy', 'roc_auc', 'f1_score', 'precision']]\n",
    "\n",
    "print('\\n', leaderboard.to_string())\n",
    "\n",
    "# Save leaderboard\n",
    "leaderboard.to_csv('outputs/final_leaderboard_with_tuning.csv')\n",
    "print('\\n\u2705 Leaderboard saved to: outputs/final_leaderboard_with_tuning.csv')\n",
    "\n",
    "# Visualize top 10 models\n",
    "top_10 = leaderboard.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "x = np.arange(len(top_10))\n",
    "\n",
    "ax.barh(x, top_10['roc_auc'], color='#2ecc71', alpha=0.8)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top_10.index, fontsize=11)\n",
    "ax.set_xlabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 Models - ROC-AUC Performance', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_10['roc_auc']):\n",
    "    ax.text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/final_leaderboard_top10.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 Leaderboard visualization saved to: figures/final_leaderboard_top10.png')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('\ud83c\udf89 COMPLETE PIPELINE FINISHED SUCCESSFULLY!')\n",
    "print('='*80)\n",
    "print(f'\\n\ud83c\udfc6 BEST MODEL: {leaderboard.index[0]}')\n",
    "print(f'\ud83d\udcca ROC-AUC: {leaderboard.iloc[0][\"roc_auc\"]:.4f}')\n",
    "print(f'\ud83c\udfaf Accuracy: {leaderboard.iloc[0][\"accuracy\"]:.4f}')\n",
    "print(f'\ud83d\udcaf F1-Score: {leaderboard.iloc[0][\"f1_score\"]:.4f}')\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# SECTION 10.2: SAVE BEST MODEL AS .PKL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd0d INTELLIGENT MODEL SELECTION (Multi-Criteria)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate comprehensive scoring for each model\n",
    "model_scores = {}\n",
    "\n",
    "for model_name in results.keys():\n",
    "    metrics = results[model_name]\n",
    "\n",
    "    # Get model for overfitting check\n",
    "    model = trained_models.get(model_name)\n",
    "\n",
    "    # Calculate train accuracy to check overfitting\n",
    "    if model_name == 'Neural Network':\n",
    "        y_train_pred = (model.predict(X_train, verbose=0) > 0.5).astype(int).flatten()\n",
    "    else:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = metrics['accuracy']\n",
    "\n",
    "    # Calculate overfitting penalty (train - test gap)\n",
    "    overfitting_gap = abs(train_accuracy - test_accuracy)\n",
    "    overfitting_penalty = overfitting_gap * 2  # Penalize 2x\n",
    "\n",
    "    # Multi-criteria composite score\n",
    "    score = (\n",
    "        metrics['roc_auc'] * 0.35 +           # ROC-AUC: 35% weight (most important)\n",
    "        metrics['accuracy'] * 0.25 +           # Accuracy: 25% weight\n",
    "        metrics['f1_score'] * 0.20 +           # F1-Score: 20% weight\n",
    "        metrics['precision'] * 0.10 +          # Precision: 10% weight\n",
    "        (1 - overfitting_penalty) * 0.10       # Overfitting check: 10% weight\n",
    "    )\n",
    "\n",
    "    model_scores[model_name] = {\n",
    "        'composite_score': score,\n",
    "        'roc_auc': metrics['roc_auc'],\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_score': metrics['f1_score'],\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'is_optimized': 'Optimized' in model_name or 'Tuned' in model_name\n",
    "    }\n",
    "\n",
    "# Sort by composite score\n",
    "ranked_models = sorted(model_scores.items(), key=lambda x: x[1]['composite_score'], reverse=True)\n",
    "\n",
    "# Display ranking\n",
    "print(\"\\n\ud83c\udfc6 MODEL RANKING (Multi-Criteria Composite Score):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<6} {'Model':<30} {'Score':<8} {'ROC-AUC':<9} {'Accuracy':<9} {'Overfit':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (model_name, scores) in enumerate(ranked_models[:10], 1):\n",
    "    print(f\"{i:<6} {model_name:<30} {scores['composite_score']:.4f}   \"\n",
    "          f\"{scores['roc_auc']:.4f}    {scores['accuracy']:.4f}    \"\n",
    "          f\"{scores['overfitting_gap']:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = ranked_models[0][0]\n",
    "best_scores = ranked_models[0][1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 SELECTED BEST MODEL (Intelligent Multi-Criteria Selection)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   Composite Score: {best_scores['composite_score']:.4f}\")\n",
    "print(f\"   ROC-AUC: {best_scores['roc_auc']:.4f}\")\n",
    "print(f\"   Accuracy: {best_scores['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_scores['f1_score']:.4f}\")\n",
    "print(f\"   Train Accuracy: {best_scores['train_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {best_scores['overfitting_gap']:.4f}\")\n",
    "print(f\"   Optimized: {'Yes' if best_scores['is_optimized'] else 'No'}\")\n",
    "\n",
    "# Additional validation for optimized models\n",
    "if not best_scores['is_optimized']:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: Best model is not optimized!\")\n",
    "    print(\"   Checking if an optimized version exists in top 3...\")\n",
    "\n",
    "    for rank, (model_name, scores) in enumerate(ranked_models[:3], 1):\n",
    "        if scores['is_optimized']:\n",
    "            print(f\"   \u2192 Found optimized model at rank {rank}: {model_name}\")\n",
    "            print(f\"   \u2192 Score difference: {best_scores['composite_score'] - scores['composite_score']:.4f}\")\n",
    "\n",
    "            # If score difference is small (<0.01), prefer optimized version\n",
    "            if (best_scores['composite_score'] - scores['composite_score']) < 0.01:\n",
    "                print(f\"   \u2192 Selecting {model_name} instead (negligible score difference)\")\n",
    "                best_model_name = model_name\n",
    "                best_scores = scores\n",
    "            break\n",
    "\n",
    "# Save the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Create filename\n",
    "model_filename = f\"{best_model_name.replace(' ', '_').lower()}_best.pkl\"\n",
    "model_path = f\"models/{model_filename}\"\n",
    "\n",
    "# Save the best model using pickle\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Best model saved successfully!\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   ROC-AUC: {best_scores['roc_auc']:.4f}\")\n",
    "print(f\"   Path: {model_path}\")\n",
    "\n",
    "# Save the scaler for deployment\n",
    "scaler_path = \"models/scaler.pkl\"\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"\\n\u2705 Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save label encoders dictionary\n",
    "encoders_path = \"models/label_encoders.pkl\"\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(le_dict, f)\n",
    "\n",
    "print(f\"\u2705 Label encoders saved to: {encoders_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udce6 MODEL ARTIFACTS SAVED - READY FOR DEPLOYMENT!\")\n",
    "print(\"=\"*80)\n",
    "print(\"Files saved:\")\n",
    "print(f\"   1. {model_path} (Best ML model)\")\n",
    "print(f\"   2. {scaler_path} (Feature scaler)\")\n",
    "print(f\"   3. {encoders_path} (Categorical encoders)\")\n",
    "print(\"\\nYou can now use these files to make predictions on new data!\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "id": "7bwpS6YmAkYu"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}