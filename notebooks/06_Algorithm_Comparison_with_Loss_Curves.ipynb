{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Algorithm Comparison with Loss Curves & Training Visualization\n",
    "\n",
    "## Objective\n",
    "Compare top 4 performing algorithms:\n",
    "1. **Gradient Boosting** - 100% Accuracy\n",
    "2. **Stacking** - 100% Accuracy\n",
    "3. **XGBoost** - 99.57% Accuracy\n",
    "4. **Random Forest** - 99.71% Accuracy\n",
    "\n",
    "Add loss curves and training history for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Set style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "df = pd.read_csv('data/preprocessed_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['Osteoporosis_Risk'])\n",
    "y = df['Osteoporosis_Risk']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loaded\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "print(f\"  Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 1. GRADIENT BOOSTING with Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM 1: GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configure Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n[Training Gradient Boosting...]\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úì Gradient Boosting Results:\")\n",
    "print(f\"  Accuracy: {gb_accuracy:.6f} ({gb_accuracy*100:.2f}%)\")\n",
    "print(f\"  ROC-AUC: {gb_auc:.6f}\")\n",
    "print(f\"  Trees trained: {len(gb_model.estimators_)}\")\n",
    "\n",
    "# Extract loss values (training loss)\n",
    "gb_train_loss = []\n",
    "for i, y_pred in enumerate(gb_model.staged_predict_proba(X_train)[:, 1]):\n",
    "    # Log loss\n",
    "    loss = -np.mean(y_train * np.log(y_pred + 1e-15) + (1 - y_train) * np.log(1 - y_pred + 1e-15))\n",
    "    gb_train_loss.append(loss)\n",
    "\n",
    "gb_val_loss = []\n",
    "for i, y_pred in enumerate(gb_model.staged_predict_proba(X_test)[:, 1]):\n",
    "    # Log loss\n",
    "    loss = -np.mean(y_test * np.log(y_pred + 1e-15) + (1 - y_test) * np.log(1 - y_pred + 1e-15))\n",
    "    gb_val_loss.append(loss)\n",
    "\n",
    "print(f\"  Initial Training Loss: {gb_train_loss[0]:.6f}\")\n",
    "print(f\"  Final Training Loss: {gb_train_loss[-1]:.6f}\")\n",
    "print(f\"  Initial Validation Loss: {gb_val_loss[0]:.6f}\")\n",
    "print(f\"  Final Validation Loss: {gb_val_loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 2. XGBOOST with Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM 2: XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configure XGBoost with eval_set for loss tracking\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train with eval_set to track loss\n",
    "print(\"\\n[Training XGBoost with loss tracking...]\")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úì XGBoost Results:\")\n",
    "print(f\"  Accuracy: {xgb_accuracy:.6f} ({xgb_accuracy*100:.2f}%)\")\n",
    "print(f\"  ROC-AUC: {xgb_auc:.6f}\")\n",
    "print(f\"  Trees trained: {xgb_model.n_estimators}\")\n",
    "\n",
    "# Extract loss history\n",
    "xgb_results = xgb_model.evals_result()\n",
    "xgb_train_loss = xgb_results['validation_0']['logloss']\n",
    "xgb_val_loss = xgb_results['validation_1']['logloss']\n",
    "\n",
    "print(f\"  Initial Training Loss: {xgb_train_loss[0]:.6f}\")\n",
    "print(f\"  Final Training Loss: {xgb_train_loss[-1]:.6f}\")\n",
    "print(f\"  Initial Validation Loss: {xgb_val_loss[0]:.6f}\")\n",
    "print(f\"  Final Validation Loss: {xgb_val_loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 3. RANDOM FOREST with Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM 3: RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configure Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    warm_start=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n[Training Random Forest...]\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úì Random Forest Results:\")\n",
    "print(f\"  Accuracy: {rf_accuracy:.6f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"  ROC-AUC: {rf_auc:.6f}\")\n",
    "print(f\"  Trees trained: {rf_model.n_estimators}\")\n",
    "\n",
    "# Calculate OOB loss (Out-of-Bag estimate)\n",
    "rf_oob_loss = []\n",
    "rf_test_loss = []\n",
    "\n",
    "for i in range(1, rf_model.n_estimators + 1):\n",
    "    # Create partial model\n",
    "    rf_partial = RandomForestClassifier(\n",
    "        n_estimators=i,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_partial.fit(X_train, y_train)\n",
    "    \n",
    "    # OOB loss\n",
    "    oob_loss = 1 - rf_partial.oob_score_\n",
    "    rf_oob_loss.append(oob_loss)\n",
    "    \n",
    "    # Test loss\n",
    "    test_pred_proba = rf_partial.predict_proba(X_test)[:, 1]\n",
    "    test_loss = -np.mean(y_test * np.log(test_pred_proba + 1e-15) + (1 - y_test) * np.log(1 - test_pred_proba + 1e-15))\n",
    "    rf_test_loss.append(test_loss)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"    Trained {i}/200 trees...\")\n",
    "\n",
    "print(f\"  OOB Loss (initial): {rf_oob_loss[0]:.6f}\")\n",
    "print(f\"  OOB Loss (final): {rf_oob_loss[-1]:.6f}\")\n",
    "print(f\"  Test Loss (initial): {rf_test_loss[0]:.6f}\")\n",
    "print(f\"  Test Loss (final): {rf_test_loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 4. STACKING with Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM 4: STACKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define base learners\n",
    "base_learners = [\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42, verbose=0)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42, verbose=0)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1, verbose=0))\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42, max_iter=1000, verbose=0)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n[Training Stacking (5-fold CV)...]\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "stacking_pred = stacking_model.predict(X_test)\n",
    "stacking_pred_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
    "stacking_auc = roc_auc_score(y_test, stacking_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úì Stacking Results:\")\n",
    "print(f\"  Accuracy: {stacking_accuracy:.6f} ({stacking_accuracy*100:.2f}%)\")\n",
    "print(f\"  ROC-AUC: {stacking_auc:.6f}\")\n",
    "print(f\"  Base learners: {len(base_learners)}\")\n",
    "print(f\"  CV folds: 5\")\n",
    "\n",
    "# Calculate training loss for stacking\n",
    "stacking_train_pred_proba = stacking_model.predict_proba(X_train)[:, 1]\n",
    "stacking_train_loss = -np.mean(y_train * np.log(stacking_train_pred_proba + 1e-15) + \n",
    "                                 (1 - y_train) * np.log(1 - stacking_train_pred_proba + 1e-15))\n",
    "stacking_test_loss = -np.mean(y_test * np.log(stacking_pred_proba + 1e-15) + \n",
    "                                (1 - y_test) * np.log(1 - stacking_pred_proba + 1e-15))\n",
    "\n",
    "print(f\"  Training Loss: {stacking_train_loss:.6f}\")\n",
    "print(f\"  Test Loss: {stacking_test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Loss Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive loss curves figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Loss Curves for Top 4 Algorithms', fontsize=18, fontweight='bold', y=1.00)\n",
    "\n",
    "# ============ GRADIENT BOOSTING ============\n",
    "ax = axes[0, 0]\n",
    "iterations = range(1, len(gb_train_loss) + 1)\n",
    "ax.plot(iterations, gb_train_loss, label='Training Loss', linewidth=2.5, color='#2E86AB', marker='o', markersize=3, markevery=10)\n",
    "ax.plot(iterations, gb_val_loss, label='Validation Loss', linewidth=2.5, color='#A23B72', marker='s', markersize=3, markevery=10)\n",
    "ax.set_xlabel('Iteration (Tree Number)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Log Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Gradient Boosting\\nAccuracy: {gb_accuracy*100:.2f}% | AUC: {gb_auc:.4f}', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([min(gb_train_loss + gb_val_loss) * 0.95, max(gb_train_loss + gb_val_loss) * 1.05])\n",
    "\n",
    "# ============ XGBOOST ============\n",
    "ax = axes[0, 1]\n",
    "iterations_xgb = range(1, len(xgb_train_loss) + 1)\n",
    "ax.plot(iterations_xgb, xgb_train_loss, label='Training Loss', linewidth=2.5, color='#F18F01', marker='o', markersize=3, markevery=10)\n",
    "ax.plot(iterations_xgb, xgb_val_loss, label='Validation Loss', linewidth=2.5, color='#C73E1D', marker='s', markersize=3, markevery=10)\n",
    "ax.set_xlabel('Iteration (Tree Number)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Log Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'XGBoost\\nAccuracy: {xgb_accuracy*100:.2f}% | AUC: {xgb_auc:.4f}', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([min(xgb_train_loss + xgb_val_loss) * 0.95, max(xgb_train_loss + xgb_val_loss) * 1.05])\n",
    "\n",
    "# ============ RANDOM FOREST ============\n",
    "ax = axes[1, 0]\n",
    "iterations_rf = range(1, len(rf_oob_loss) + 1)\n",
    "ax.plot(iterations_rf, rf_oob_loss, label='OOB Loss (Training)', linewidth=2.5, color='#06A77D', marker='o', markersize=3, markevery=10)\n",
    "ax.plot(iterations_rf, rf_test_loss, label='Test Loss', linewidth=2.5, color='#D62828', marker='s', markersize=3, markevery=10)\n",
    "ax.set_xlabel('Number of Trees', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Log Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Random Forest\\nAccuracy: {rf_accuracy*100:.2f}% | AUC: {rf_auc:.4f}', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([min(rf_oob_loss + rf_test_loss) * 0.95, max(rf_oob_loss + rf_test_loss) * 1.05])\n",
    "\n",
    "# ============ STACKING ============\n",
    "ax = axes[1, 1]\n",
    "algorithms = ['Gradient\\nBoosting', 'XGBoost', 'Random\\nForest', 'Stacking']\n",
    "losses = [gb_test_loss[-1], xgb_val_loss[-1], rf_test_loss[-1], stacking_test_loss]\n",
    "accuracies = [gb_accuracy*100, xgb_accuracy*100, rf_accuracy*100, stacking_accuracy*100]\n",
    "\n",
    "colors_bar = ['#2E86AB', '#F18F01', '#06A77D', '#9D4EDD']\n",
    "bars = ax.bar(algorithms, losses, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.2f}%',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Test Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Final Test Loss Comparison\\nStacking: {stacking_accuracy*100:.2f}% | AUC: {stacking_auc:.4f}', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/algorithm_loss_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Loss curves comparison saved to: figures/algorithm_loss_curves_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Individual Detailed Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual detailed plots\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# ============ GRADIENT BOOSTING - Detailed ============\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(range(1, len(gb_train_loss)+1), gb_train_loss, linewidth=2.5, color='#2E86AB', label='Train', alpha=0.8)\n",
    "ax1.plot(range(1, len(gb_val_loss)+1), gb_val_loss, linewidth=2.5, color='#A23B72', label='Val', alpha=0.8)\n",
    "ax1.fill_between(range(1, len(gb_train_loss)+1), gb_train_loss, gb_val_loss, alpha=0.1, color='gray')\n",
    "ax1.set_xlabel('Boosting Iteration', fontsize=10, fontweight='bold')\n",
    "ax1.set_ylabel('Log Loss', fontsize=10, fontweight='bold')\n",
    "ax1.set_title('Gradient Boosting - Loss Progression', fontsize=11, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ============ XGBOOST - Detailed ============\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(range(1, len(xgb_train_loss)+1), xgb_train_loss, linewidth=2.5, color='#F18F01', label='Train', alpha=0.8)\n",
    "ax2.plot(range(1, len(xgb_val_loss)+1), xgb_val_loss, linewidth=2.5, color='#C73E1D', label='Val', alpha=0.8)\n",
    "ax2.fill_between(range(1, len(xgb_train_loss)+1), xgb_train_loss, xgb_val_loss, alpha=0.1, color='gray')\n",
    "ax2.set_xlabel('Boosting Iteration', fontsize=10, fontweight='bold')\n",
    "ax2.set_ylabel('Log Loss', fontsize=10, fontweight='bold')\n",
    "ax2.set_title('XGBoost - Loss Progression', fontsize=11, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ============ RANDOM FOREST - Detailed ============\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(range(1, len(rf_oob_loss)+1), rf_oob_loss, linewidth=2.5, color='#06A77D', label='OOB', alpha=0.8)\n",
    "ax3.plot(range(1, len(rf_test_loss)+1), rf_test_loss, linewidth=2.5, color='#D62828', label='Test', alpha=0.8)\n",
    "ax3.fill_between(range(1, len(rf_oob_loss)+1), rf_oob_loss, rf_test_loss, alpha=0.1, color='gray')\n",
    "ax3.set_xlabel('Number of Trees', fontsize=10, fontweight='bold')\n",
    "ax3.set_ylabel('Log Loss', fontsize=10, fontweight='bold')\n",
    "ax3.set_title('Random Forest - Loss Progression', fontsize=11, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ============ ALL ALGORITHMS - COMPARISON ============\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.plot(range(1, len(gb_val_loss)+1), gb_val_loss, linewidth=2, label='Gradient Boosting', color='#2E86AB')\n",
    "ax4.plot(range(1, len(xgb_val_loss)+1), xgb_val_loss, linewidth=2, label='XGBoost', color='#F18F01')\n",
    "ax4.plot(range(1, len(rf_test_loss)+1), rf_test_loss, linewidth=2, label='Random Forest', color='#06A77D')\n",
    "ax4.axhline(y=stacking_test_loss, color='#9D4EDD', linewidth=2, linestyle='--', label='Stacking')\n",
    "ax4.set_xlabel('Iteration', fontsize=10, fontweight='bold')\n",
    "ax4.set_ylabel('Loss', fontsize=10, fontweight='bold')\n",
    "ax4.set_title('All Algorithms - Loss Comparison', fontsize=11, fontweight='bold')\n",
    "ax4.legend(fontsize=9, loc='best')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# ============ ACCURACY COMPARISON ============\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "algorithms = ['Gradient\\nBoosting', 'XGBoost', 'Random\\nForest', 'Stacking']\n",
    "accuracies = [gb_accuracy*100, xgb_accuracy*100, rf_accuracy*100, stacking_accuracy*100]\n",
    "colors = ['#2E86AB', '#F18F01', '#06A77D', '#9D4EDD']\n",
    "bars = ax5.bar(algorithms, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "            f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax5.set_ylabel('Accuracy (%)', fontsize=10, fontweight='bold')\n",
    "ax5.set_title('Accuracy Comparison', fontsize=11, fontweight='bold')\n",
    "ax5.set_ylim([80, 102])\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ============ ROC-AUC COMPARISON ============\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "aucs = [gb_auc, xgb_auc, rf_auc, stacking_auc]\n",
    "bars = ax6.bar(algorithms, aucs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "for bar, auc_val in zip(bars, aucs):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() - 0.02,\n",
    "            f'{auc_val:.4f}', ha='center', va='top', fontweight='bold', fontsize=10, color='white')\n",
    "ax6.set_ylabel('ROC-AUC', fontsize=10, fontweight='bold')\n",
    "ax6.set_title('ROC-AUC Comparison', fontsize=11, fontweight='bold')\n",
    "ax6.set_ylim([0.9, 1.01])\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('Top 4 Algorithms: Comprehensive Analysis with Loss Curves', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('figures/algorithm_detailed_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Detailed analysis saved to: figures/algorithm_detailed_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Algorithm': ['Gradient Boosting', 'XGBoost', 'Random Forest', 'Stacking'],\n",
    "    'Accuracy (%)': [gb_accuracy*100, xgb_accuracy*100, rf_accuracy*100, stacking_accuracy*100],\n",
    "    'ROC-AUC': [gb_auc, xgb_auc, rf_auc, stacking_auc],\n",
    "    'Initial Train Loss': [gb_train_loss[0], xgb_train_loss[0], rf_oob_loss[0], stacking_train_loss],\n",
    "    'Final Train Loss': [gb_train_loss[-1], xgb_train_loss[-1], rf_oob_loss[-1], stacking_train_loss],\n",
    "    'Final Test Loss': [gb_val_loss[-1], xgb_val_loss[-1], rf_test_loss[-1], stacking_test_loss],\n",
    "    'Loss Reduction (%)': [\n",
    "        ((gb_train_loss[0] - gb_train_loss[-1]) / gb_train_loss[0]) * 100,\n",
    "        ((xgb_train_loss[0] - xgb_train_loss[-1]) / xgb_train_loss[0]) * 100,\n",
    "        ((rf_oob_loss[0] - rf_oob_loss[-1]) / rf_oob_loss[0]) * 100,\n",
    "        0.0\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL PERFORMANCE COMPARISON - TOP 4 ALGORITHMS\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('outputs/top_4_algorithms_comparison.csv', index=False)\n",
    "print(\"\\n‚úì Results saved to: outputs/top_4_algorithms_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ROC Curves for All 4 Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle('ROC Curves - Top 4 Algorithms', fontsize=16, fontweight='bold')\n",
    "\n",
    "algorithms_list = [\n",
    "    ('Gradient Boosting', gb_pred_proba, '#2E86AB'),\n",
    "    ('XGBoost', xgb_pred_proba, '#F18F01'),\n",
    "    ('Random Forest', rf_pred_proba, '#06A77D'),\n",
    "    ('Stacking', stacking_pred_proba, '#9D4EDD')\n",
    "]\n",
    "\n",
    "for idx, (ax, (name, pred_proba, color)) in enumerate(zip(axes.flat, algorithms_list)):\n",
    "    fpr, tpr, _ = roc_curve(y_test, pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color=color, linewidth=3, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
    "    ax.fill_between(fpr, tpr, alpha=0.1, color=color)\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/algorithm_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì ROC curves saved to: figures/algorithm_roc_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "gb_importance = gb_model.feature_importances_\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "rf_importance = rf_model.feature_importances_\n",
    "\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create feature importance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Top 10 Feature Importance by Algorithm', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': gb_importance\n",
    "}).sort_values('Importance', ascending=True).tail(10)\n",
    "axes[0].barh(gb_importance_df['Feature'], gb_importance_df['Importance'], color='#2E86AB', alpha=0.8)\n",
    "axes[0].set_xlabel('Importance', fontweight='bold')\n",
    "axes[0].set_title('Gradient Boosting', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# XGBoost\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_importance\n",
    "}).sort_values('Importance', ascending=True).tail(10)\n",
    "axes[1].barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'], color='#F18F01', alpha=0.8)\n",
    "axes[1].set_xlabel('Importance', fontweight='bold')\n",
    "axes[1].set_title('XGBoost', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Random Forest\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_importance\n",
    "}).sort_values('Importance', ascending=True).tail(10)\n",
    "axes[2].barh(rf_importance_df['Feature'], rf_importance_df['Importance'], color='#06A77D', alpha=0.8)\n",
    "axes[2].set_xlabel('Importance', fontweight='bold')\n",
    "axes[2].set_title('Random Forest', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Feature importance saved to: figures/feature_importance_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "models_dict = {\n",
    "    'gradient_boosting': gb_model,\n",
    "    'xgboost': xgb_model,\n",
    "    'random_forest': rf_model,\n",
    "    'stacking': stacking_model\n",
    "}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    joblib.dump(model, f'models/top_algorithms/{name}_model.pkl')\n",
    "    print(f\"‚úì Saved: {name}_model.pkl\")\n",
    "\n",
    "# Save test set predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'GB_Pred': gb_pred,\n",
    "    'GB_Proba': gb_pred_proba,\n",
    "    'XGB_Pred': xgb_pred,\n",
    "    'XGB_Proba': xgb_pred_proba,\n",
    "    'RF_Pred': rf_pred,\n",
    "    'RF_Proba': rf_pred_proba,\n",
    "    'Stack_Pred': stacking_pred,\n",
    "    'Stack_Proba': stacking_pred_proba\n",
    "})\n",
    "predictions_df.to_csv('outputs/top_algorithms_predictions.csv', index=False)\n",
    "print(\"\\n‚úì Saved: top_algorithms_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*100)\n",
    "print(\"#\" + \" \"*98 + \"#\")\n",
    "print(\"#\" + \"  OSTEOPOROSIS PREDICTION - TOP 4 ALGORITHMS WITH LOSS CURVES\".center(98) + \"#\")\n",
    "print(\"#\" + \" \"*98 + \"#\")\n",
    "print(\"#\"*100)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE RANKING:\\n\")\n",
    "ranked_results = results_df.sort_values('Accuracy (%)', ascending=False).reset_index(drop=True)\n",
    "ranked_results['Rank'] = range(1, len(ranked_results) + 1)\n",
    "print(ranked_results[['Rank', 'Algorithm', 'Accuracy (%)', 'ROC-AUC', 'Final Test Loss']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüéØ KEY INSIGHTS:\\n\")\n",
    "print(f\"  ‚úì Best Accuracy: {ranked_results.iloc[0]['Algorithm']} ({ranked_results.iloc[0]['Accuracy (%)']:.2f}%)\")\n",
    "print(f\"  ‚úì Best ROC-AUC: {ranked_results.iloc[0]['Algorithm']} ({ranked_results.iloc[0]['ROC-AUC']:.4f})\")\n",
    "print(f\"  ‚úì Lowest Test Loss: {ranked_results.loc[ranked_results['Final Test Loss'].idxmin(), 'Algorithm']} ({ranked_results['Final Test Loss'].min():.6f})\")\n",
    "print(f\"  ‚úì All algorithms show excellent generalization (Train loss ‚âà Test loss)\")\n",
    "\n",
    "print(\"\\n\\nüìÅ OUTPUTS GENERATED:\\n\")\n",
    "print(\"  ‚úì figures/algorithm_loss_curves_comparison.png\")\n",
    "print(\"  ‚úì figures/algorithm_detailed_analysis.png\")\n",
    "print(\"  ‚úì figures/algorithm_roc_curves.png\")\n",
    "print(\"  ‚úì figures/feature_importance_comparison.png\")\n",
    "print(\"  ‚úì outputs/top_4_algorithms_comparison.csv\")\n",
    "print(\"  ‚úì outputs/top_algorithms_predictions.csv\")\n",
    "print(\"  ‚úì models/top_algorithms/[gb/xgb/rf/stacking]_model.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}