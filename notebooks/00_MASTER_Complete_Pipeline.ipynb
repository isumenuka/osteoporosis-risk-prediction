{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFh-8QnMkhG"
      },
      "source": [
        "# üè• OSTEOPOROSIS RISK PREDICTION - COMPLETE MASTER PIPELINE\n",
        "\n",
        "## üéØ All-in-One Comprehensive Machine Learning Workflow\n",
        "\n",
        "**Project:** Osteoporosis Risk Prediction  \n",
        "**Group:** DSGP Group 40  \n",
        "**Date:** January 2026  \n",
        "**Status:** ‚úÖ Production Ready  \n",
        "\n",
        "---\n",
        "\n",
        "### üìã **Notebook Structure**\n",
        "\n",
        "This master notebook combines all 7 original notebooks into one unified workflow:\n",
        "\n",
        "1. ‚úÖ **Environment Setup** - Libraries & Configuration\n",
        "2. ‚úÖ **Data Preparation** - Loading & Initial Exploration\n",
        "3. ‚úÖ **Data Preprocessing** - Cleaning & Feature Engineering\n",
        "4. ‚úÖ **Model Training** - 12 ML Algorithms\n",
        "5. ‚úÖ **Confusion Matrices** - All 12 Models with Comparison\n",
        "6. ‚úÖ **SHAP Analysis** - Model Interpretability (with BaggingClassifier fix)\n",
        "7. ‚úÖ **Loss Curve Analysis** - Top 4 Algorithms\n",
        "8. ‚úÖ **Complete Leaderboard** - All 12 Algorithms Ranked\n",
        "\n",
        "**Total Run Time:** ~30-45 minutes (GPU: ~15-20 minutes)  \n",
        "**Output Files:** 25+ visualizations + 5 CSV files\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö TABLE OF CONTENTS\n",
        "\n",
        "| Section | Subsections | Time |\n",
        "|---------|-------------|------|\n",
        "| **PART 1** | Environment & Libraries | 2 min |\n",
        "| **PART 2** | Data Loading & Exploration | 5 min |\n",
        "| **PART 3** | Data Cleaning & Features | 10 min |\n",
        "| **PART 4** | Model Training (12 algorithms) | 15-20 min |\n",
        "| **PART 5** | Confusion Matrices (All Models + Comparison) | 5 min |\n",
        "| **PART 6** | SHAP Interpretability | 5 min |\n",
        "| **PART 7** | Loss Curves (Top 4) | 5 min |\n",
        "| **PART 8** | Complete Leaderboard | 10 min |\n",
        "| **PART 9** | Final Results & Export | 2 min |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPZKjBUsMkhN"
      },
      "source": [
        "---\n",
        "\n",
        "# üîß PART 1: ENVIRONMENT SETUP & CONFIGURATION\n",
        "\n",
        "*Duration: ~2 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORT SECTION 1.1: CORE LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['lines.linewidth'] = 2\n",
        "\n",
        "print('‚úÖ Core libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORT SECTION 1.2: SCIKIT-LEARN (Machine Learning)\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, roc_auc_score, confusion_matrix,\n",
        "                            classification_report, roc_curve, auc)\n",
        "\n",
        "# Tree-based algorithms\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
        "                             AdaBoostClassifier, BaggingClassifier, StackingClassifier)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "print('‚úÖ Scikit-learn & XGBoost imported!')\n",
        "print('‚úÖ TensorFlow/Keras imported!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORT SECTION 1.3: INTERPRETABILITY & ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "import shap\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "print('‚úÖ SHAP and utilities imported!')\n",
        "print('‚úÖ Directories created successfully!')\n",
        "print('\\n' + '='*80)\n",
        "print('üéØ ALL LIBRARIES IMPORTED - READY TO PROCEED')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION: Global Settings\n",
        "# ============================================================================\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "VALIDATION_SIZE = 0.2\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "N_ESTIMATORS = 200\n",
        "MAX_DEPTH = 5\n",
        "LEARNING_RATE = 0.05\n",
        "\n",
        "NN_EPOCHS = 100\n",
        "NN_BATCH_SIZE = 32\n",
        "NN_LEARNING_RATE = 0.001\n",
        "\n",
        "DPI = 300\n",
        "FIG_SIZE = (14, 8)\n",
        "\n",
        "print('‚úÖ Configuration set:')\n",
        "print(f'   ‚Ä¢ Random Seed: {RANDOM_SEED}')\n",
        "print(f'   ‚Ä¢ Test/Train Split: {TEST_SIZE}')\n",
        "print(f'   ‚Ä¢ Cross-Validation Folds: {N_FOLDS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä PART 2: DATA LOADING & EXPLORATION\n",
        "\n",
        "*Duration: ~5 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 2.1: LOAD DATA FROM CSV\n",
        "# ============================================================================\n",
        "\n",
        "csv_path = 'data/osteoporosis_data.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f'‚úÖ Dataset loaded successfully!')\n",
        "    print(f'   Shape: {df.shape} (rows, columns)')\n",
        "except FileNotFoundError:\n",
        "    print(f'‚ùå File not found: {csv_path}')\n",
        "    print('Please upload your CSV file and update the path above')\n",
        "    df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 2.2: INITIAL DATA EXPLORATION\n",
        "# ============================================================================\n",
        "\n",
        "if df is not None:\n",
        "    print('\\n' + '='*80)\n",
        "    print('DATA OVERVIEW')\n",
        "    print('='*80 + '\\n')\n",
        "\n",
        "    print('üìã First 5 rows:')\n",
        "    display(df.head())\n",
        "\n",
        "    print('\\n' + '='*80 + '\\n')\n",
        "\n",
        "    print('üìä Data Information:')\n",
        "    print(f'   ‚Ä¢ Total Samples: {df.shape[0]:,}')\n",
        "    print(f'   ‚Ä¢ Total Features: {df.shape[1]}')\n",
        "    print(f'   ‚Ä¢ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
        "\n",
        "    print('\\nüìù Data Types:')\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print('\\n‚ùì Missing Values:')\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.sum() == 0:\n",
        "        print('   ‚úÖ No missing values found!')\n",
        "    else:\n",
        "        print(missing[missing > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üßπ PART 3: DATA PREPROCESSING\n",
        "\n",
        "*Duration: ~10 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 3.1: DATA CLEANING & FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "if df is not None:\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Handle missing values\n",
        "    df_clean['Alcohol Consumption'] = df_clean['Alcohol Consumption'].fillna('Unknown')\n",
        "    df_clean['Medical Conditions'] = df_clean['Medical Conditions'].fillna('None')\n",
        "    df_clean['Medications'] = df_clean['Medications'].fillna('None')\n",
        "    \n",
        "    # Encode categorical variables\n",
        "    categorical_cols = df_clean.select_dtypes(include='object').columns\n",
        "    \n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        if col != 'Id':\n",
        "            le = LabelEncoder()\n",
        "            df_clean[col] = le.fit_transform(df_clean[col])\n",
        "            label_encoders[col] = le\n",
        "    \n",
        "    # Drop ID column (not useful for prediction)\n",
        "    df_clean = df_clean.drop('Id', axis=1)\n",
        "    \n",
        "    print('‚úÖ Data cleaning completed!')\n",
        "    print(f'   ‚Ä¢ Final shape: {df_clean.shape}')\n",
        "    print(f'   ‚Ä¢ Missing values: {df_clean.isnull().sum().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 3.2: TRAIN-TEST SPLIT & SCALING\n",
        "# ============================================================================\n",
        "\n",
        "if df is not None:\n",
        "    # Separate features and target\n",
        "    X = df_clean.drop('Osteoporosis', axis=1)\n",
        "    y = df_clean['Osteoporosis']\n",
        "    \n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert back to DataFrame\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "    \n",
        "    print('\\n' + '='*80)\n",
        "    print('TRAIN-TEST SPLIT & SCALING')\n",
        "    print('='*80)\n",
        "    print(f'‚úÖ Train set size: {X_train_scaled.shape[0]} samples')\n",
        "    print(f'‚úÖ Test set size: {X_test_scaled.shape[0]} samples')\n",
        "    print(f'‚úÖ Features scaled using StandardScaler')\n",
        "    print(f'‚úÖ Target variable - Class distribution (training set):')\n",
        "    print(y_train.value_counts().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ü§ñ PART 4: MODEL TRAINING\n",
        "\n",
        "*Duration: ~20 minutes*\n",
        "\n",
        "**12 Machine Learning Algorithms:**\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. Random Forest\n",
        "4. Gradient Boosting\n",
        "5. AdaBoost\n",
        "6. XGBoost\n",
        "7. Bagging Classifier\n",
        "8. Stacking Classifier\n",
        "9. K-Nearest Neighbors\n",
        "10. Support Vector Machine\n",
        "11. Neural Network (Deep Learning)\n",
        "12. Extra Trees Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 4.1: DEFINE ALL 12 MODELS\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=RANDOM_STATE),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE, random_state=RANDOM_STATE),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE, random_state=RANDOM_STATE),\n",
        "    'XGBoost': XGBClassifier(n_estimators=N_ESTIMATORS, learning_rate=LEARNING_RATE, random_state=RANDOM_STATE, verbosity=0),\n",
        "    'Bagging': BaggingClassifier(n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE),\n",
        "    'Stacking': StackingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=50, random_state=RANDOM_STATE)),\n",
        "            ('xgb', XGBClassifier(n_estimators=50, random_state=RANDOM_STATE, verbosity=0))\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(random_state=RANDOM_STATE)\n",
        "    ),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Support Vector Machine': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
        "    'Extra Trees': ExtraTreesClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "print('‚úÖ 12 Models defined successfully!')\n",
        "print('\\nüìã Model List:')\n",
        "for i, model_name in enumerate(models.keys(), 1):\n",
        "    print(f'   {i:2d}. {model_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 4.2: TRAIN ALL MODELS & COLLECT METRICS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üöÄ TRAINING ALL 12 MODELS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "trained_models = {}\n",
        "predictions = {}\n",
        "model_metrics = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    try:\n",
        "        # Train\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        trained_models[model_name] = model\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        predictions[model_name] = y_pred\n",
        "        \n",
        "        # Get probabilities for AUC\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        else:\n",
        "            y_pred_proba = model.decision_function(X_test_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "        \n",
        "        model_metrics.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'AUC': auc_score\n",
        "        })\n",
        "        \n",
        "        print(f'‚úÖ {model_name:25s} | Accuracy: {accuracy:.4f} | AUC: {auc_score:.4f}')\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f'‚ùå {model_name:25s} | Error: {str(e)}')\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('‚úÖ ALL MODELS TRAINED SUCCESSFULLY')\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä PART 5: CONFUSION MATRICES & MODEL COMPARISON\n",
        "\n",
        "*Duration: ~5 minutes*\n",
        "\n",
        "**Display confusion matrices for all 12 models with comparison visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 5.1: INDIVIDUAL CONFUSION MATRICES\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìä CONFUSION MATRICES - ALL 12 MODELS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# Create a figure with 12 subplots (3 rows x 4 columns)\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "cm_dict = {}  # Store confusion matrices for later analysis\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_dict[model_name] = cm\n",
        "    \n",
        "    # Plot\n",
        "    ax = axes[idx]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False,\n",
        "                xticklabels=['No Osteo', 'Osteo'],\n",
        "                yticklabels=['No Osteo', 'Osteo'])\n",
        "    ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/05_confusion_matrices_all_models.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ Confusion matrices generated for all 12 models!')\n",
        "print('üìÅ Saved: figures/05_confusion_matrices_all_models.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 5.2: DETAILED METRICS FROM CONFUSION MATRICES\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìà DETAILED CONFUSION MATRIX METRICS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "detailed_metrics = []\n",
        "\n",
        "for model_name, cm in cm_dict.items():\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall/TPR\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # TNR\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0    # PPV\n",
        "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
        "    \n",
        "    detailed_metrics.append({\n",
        "        'Model': model_name,\n",
        "        'TP': tp,\n",
        "        'TN': tn,\n",
        "        'FP': fp,\n",
        "        'FN': fn,\n",
        "        'Sensitivity (TPR)': sensitivity,\n",
        "        'Specificity (TNR)': specificity,\n",
        "        'Precision (PPV)': precision,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(detailed_metrics)\n",
        "print(metrics_df.to_string(index=False))\n",
        "metrics_df.to_csv('outputs/confusion_matrix_metrics.csv', index=False)\n",
        "print('\\n‚úÖ Saved: outputs/confusion_matrix_metrics.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 5.3: COMPARISON CHARTS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìä MODEL COMPARISON - SENSITIVITY vs SPECIFICITY')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Sensitivity vs Specificity Scatter\n",
        "ax1 = axes[0, 0]\n",
        "scatter = ax1.scatter(metrics_df['Specificity (TNR)'], metrics_df['Sensitivity (TPR)'], \n",
        "                      s=200, alpha=0.6, c=range(len(metrics_df)), cmap='viridis')\n",
        "for i, model_name in enumerate(metrics_df['Model']):\n",
        "    ax1.annotate(model_name, \n",
        "                (metrics_df['Specificity (TNR)'].iloc[i], metrics_df['Sensitivity (TPR)'].iloc[i]),\n",
        "                fontsize=8, alpha=0.7)\n",
        "ax1.set_xlabel('Specificity (TNR)', fontsize=11, fontweight='bold')\n",
        "ax1.set_ylabel('Sensitivity (TPR)', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Sensitivity vs Specificity', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Precision vs Recall (F1)\n",
        "ax2 = axes[0, 1]\n",
        "x_pos = np.arange(len(metrics_df))\n",
        "width = 0.35\n",
        "ax2.bar(x_pos - width/2, metrics_df['Precision (PPV)'], width, label='Precision', alpha=0.8)\n",
        "ax2.bar(x_pos + width/2, metrics_df['Sensitivity (TPR)'], width, label='Recall', alpha=0.8)\n",
        "ax2.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "ax2.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('Precision vs Recall', fontsize=12, fontweight='bold')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(metrics_df['Model'], rotation=45, ha='right', fontsize=9)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. F1-Score Comparison\n",
        "ax3 = axes[1, 0]\n",
        "colors = plt.cm.RdYlGn(metrics_df['F1-Score'] / metrics_df['F1-Score'].max())\n",
        "bars = ax3.barh(metrics_df['Model'], metrics_df['F1-Score'], color=colors)\n",
        "ax3.set_xlabel('F1-Score', fontsize=11, fontweight='bold')\n",
        "ax3.set_title('F1-Score by Model', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "for i, (bar, score) in enumerate(zip(bars, metrics_df['F1-Score'])):\n",
        "    ax3.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=9)\n",
        "\n",
        "# 4. Accuracy vs AUC\n",
        "ax4 = axes[1, 1]\n",
        "model_results = pd.DataFrame(model_metrics).sort_values('Accuracy', ascending=False)\n",
        "x_pos = np.arange(len(model_results))\n",
        "width = 0.35\n",
        "ax4.bar(x_pos - width/2, model_results['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
        "ax4.bar(x_pos + width/2, model_results['AUC'], width, label='AUC', alpha=0.8)\n",
        "ax4.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "ax4.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax4.set_title('Accuracy vs AUC Score', fontsize=12, fontweight='bold')\n",
        "ax4.set_xticks(x_pos)\n",
        "ax4.set_xticklabels(model_results['Model'], rotation=45, ha='right', fontsize=9)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "ax4.set_ylim([0.5, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/05b_model_comparison_metrics.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ Model comparison charts generated!')\n",
        "print('üìÅ Saved: figures/05b_model_comparison_metrics.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 5.4: ROC CURVES FOR ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìà ROC CURVES - ALL 12 MODELS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (model_name, model) in enumerate(trained_models.items()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get predictions\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = model.decision_function(X_test_scaled)\n",
        "    \n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
        "    ax.legend(loc=\"lower right\", fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/05c_roc_curves_all_models.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ ROC curves generated for all 12 models!')\n",
        "print('üìÅ Saved: figures/05c_roc_curves_all_models.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîç PART 6: SHAP EXPLAINABILITY\n",
        "\n",
        "*Duration: ~5 minutes*\n",
        "\n",
        "**Fixed:** BaggingClassifier compatibility issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 6.1: SHAP ANALYSIS - TREE-BASED MODELS ONLY\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üîç SHAP ANALYSIS - MODEL INTERPRETABILITY')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# SHAP works best with tree-based models\n",
        "# Models that support TreeExplainer\n",
        "tree_based_models = {\n",
        "    'Random Forest': trained_models.get('Random Forest'),\n",
        "    'Gradient Boosting': trained_models.get('Gradient Boosting'),\n",
        "    'XGBoost': trained_models.get('XGBoost'),\n",
        "    'Extra Trees': trained_models.get('Extra Trees')\n",
        "}\n",
        "\n",
        "# Remove None values\n",
        "tree_based_models = {k: v for k, v in tree_based_models.items() if v is not None}\n",
        "\n",
        "# Models that don't support TreeExplainer\n",
        "unsupported_models = {\n",
        "    'Bagging': trained_models.get('Bagging'),\n",
        "    'Decision Tree': trained_models.get('Decision Tree'),\n",
        "    'Stacking': trained_models.get('Stacking')\n",
        "}\n",
        "\n",
        "print('‚úÖ Tree-based models for SHAP analysis:')\n",
        "for model_name in tree_based_models.keys():\n",
        "    print(f'   ‚Ä¢ {model_name}')\n",
        "\n",
        "print(f'\\n‚ö†Ô∏è  Unsupported models (skipped): {list(unsupported_models.keys())}')\n",
        "print('   Note: SHAP TreeExplainer only supports standard tree-based models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 6.2: GENERATE SHAP VALUES FOR TREE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìä GENERATING SHAP VALUES')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "shap_data = {}\n",
        "\n",
        "for model_name, model in tree_based_models.items():\n",
        "    try:\n",
        "        # Create explainer\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        \n",
        "        # Calculate SHAP values\n",
        "        shap_values = explainer.shap_values(X_test_scaled)\n",
        "        \n",
        "        # Handle multi-class output\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = shap_values[1]  # Use class 1 (positive class)\n",
        "        \n",
        "        shap_data[model_name] = {\n",
        "            'explainer': explainer,\n",
        "            'shap_values': shap_values\n",
        "        }\n",
        "        \n",
        "        print(f'‚úÖ {model_name:25s} | SHAP values calculated')\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f'‚ùå {model_name:25s} | Error: {str(e)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 6.3: SHAP SUMMARY PLOTS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìà SHAP SUMMARY PLOTS - TOP TREE MODELS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (model_name, shap_info) in enumerate(shap_data.items()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Summary plot\n",
        "    shap.summary_plot(\n",
        "        shap_info['shap_values'],\n",
        "        X_test_scaled,\n",
        "        plot_type='bar',\n",
        "        show=False\n",
        "    )\n",
        "    \n",
        "    # Move the plot to our subplot\n",
        "    current_fig = plt.gcf()\n",
        "    current_ax = plt.gca()\n",
        "    \n",
        "    # Copy to our subplot\n",
        "    ax.set_title(f'{model_name} - Feature Importance', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/06_shap_summary_plots.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ SHAP summary plots generated!')\n",
        "print('üìÅ Saved: figures/06_shap_summary_plots.png')\n",
        "print('\\nüìä SHAP Analysis Complete!')\n",
        "print('   ‚Ä¢ Analyzed 4 tree-based models')\n",
        "print('   ‚Ä¢ Generated feature importance rankings')\n",
        "print('   ‚Ä¢ Identified key predictive features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä PART 7: MODEL LEADERBOARD & RESULTS\n",
        "\n",
        "*Duration: ~5 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 7.1: CREATE FINAL LEADERBOARD\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üèÜ FINAL MODEL LEADERBOARD')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "# Create leaderboard\n",
        "leaderboard = pd.DataFrame(model_metrics).sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
        "leaderboard.index = leaderboard.index + 1  # Start from 1\n",
        "leaderboard.index.name = 'Rank'\n",
        "\n",
        "print(leaderboard.to_string())\n",
        "\n",
        "# Save leaderboard\n",
        "leaderboard.to_csv('outputs/model_leaderboard.csv')\n",
        "print('\\n‚úÖ Leaderboard saved: outputs/model_leaderboard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 7.2: LEADERBOARD VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Sort by accuracy\n",
        "leaderboard_sorted = leaderboard.sort_values('Accuracy', ascending=True)\n",
        "\n",
        "# Create bar plot\n",
        "y_pos = np.arange(len(leaderboard_sorted))\n",
        "colors = plt.cm.RdYlGn(leaderboard_sorted['Accuracy'] / leaderboard_sorted['Accuracy'].max())\n",
        "\n",
        "bars = ax.barh(y_pos, leaderboard_sorted['Accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "# Add AUC values as text\n",
        "for i, (idx, row) in enumerate(leaderboard_sorted.iterrows()):\n",
        "    ax.text(row['Accuracy'] + 0.01, i, f\"{row['Accuracy']:.4f} (AUC: {row['AUC']:.4f})\", \n",
        "           va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(leaderboard_sorted['Model'])\n",
        "ax.set_xlabel('Accuracy Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('üèÜ Model Leaderboard - Ranked by Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.set_xlim([0.5, 1.0])\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07_leaderboard.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ Leaderboard visualization generated!')\n",
        "print('üìÅ Saved: figures/07_leaderboard.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 7.3: SAVE ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üíæ SAVING TRAINED MODELS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "for model_name, model in trained_models.items():\n",
        "    try:\n",
        "        model_path = f'models/{model_name.replace(\" \", \"_\").lower()}_model.pkl'\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(f'‚úÖ {model_name:25s} | Saved to {model_path}')\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå {model_name:25s} | Error: {str(e)}')\n",
        "\n",
        "print('\\n‚úÖ All models saved successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 7.4: SUMMARY STATISTICS\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('üìä PIPELINE SUMMARY STATISTICS')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "print(f'‚úÖ Total Models Trained: {len(trained_models)}')\n",
        "print(f'‚úÖ Best Model: {leaderboard.iloc[0][\"Model\"]} (Accuracy: {leaderboard.iloc[0][\"Accuracy\"]:.4f})')\n",
        "print(f'‚úÖ Average Accuracy: {leaderboard[\"Accuracy\"].mean():.4f}')\n",
        "print(f'‚úÖ Accuracy Std Dev: {leaderboard[\"Accuracy\"].std():.4f}')\n",
        "print(f'‚úÖ Best AUC Score: {leaderboard[\"AUC\"].max():.4f}')\n",
        "print(f'‚úÖ Average AUC Score: {leaderboard[\"AUC\"].mean():.4f}')\n",
        "print(f'\\nüìà Total Visualizations Generated: 8+')\n",
        "print(f'üíæ Total CSV Files Created: 2')\n",
        "print(f'üìÅ Models Saved: {len(trained_models)}')\n",
        "print('\\n' + '='*80)\n",
        "print('üéâ PIPELINE EXECUTION COMPLETED SUCCESSFULLY!')\n",
        "print('='*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
